{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T10:06:57.295728Z",
     "start_time": "2025-01-20T10:06:56.885027Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "buETwO7OYHLN",
    "outputId": "2f3a7d1a-aa66-4665-faf1-6b70597ae209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /opt/homebrew/anaconda3/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch_geometric) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch_geometric) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch_geometric) (1.26.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->torch_geometric) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->torch_geometric) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch_geometric\n",
    "%pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f21d216EYXAh",
    "outputId": "d213c438-9cfa-4ecf-b916-e7e754790c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        2010-02-05\n",
      "1        2010-01-07\n",
      "2        2010-03-22\n",
      "3        2010-03-22\n",
      "4        2010-04-23\n",
      "            ...    \n",
      "99497    2013-04-22\n",
      "99498    2013-05-27\n",
      "99499    2013-05-22\n",
      "99500    2017-01-24\n",
      "99501    2017-03-02\n",
      "Name: DATA_PRESCRIZIONE, Length: 99502, dtype: object\n",
      "Training set: 52944 eventi\n",
      "Validation set: 33560 eventi\n",
      "Test set: 12998 eventi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/jv2fl6j10rl9sxj0731yq5q80000gn/T/ipykernel_72400/3114738972.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  paziente_diagnosi_malattia['DATA_PRESCRIZIONE'] = pd.to_datetime(paziente_diagnosi_malattia['DATA_PRESCRIZIONE'])\n",
      "/var/folders/ff/jv2fl6j10rl9sxj0731yq5q80000gn/T/ipykernel_72400/3114738972.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  paziente_diagnosi_malattia['DATA_PRESCRIZIONE'] = pd.to_datetime(paziente_diagnosi_malattia['DATA_PRESCRIZIONE'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "file_path = '/Users/salvatorebasilicata/Downloads/Uni/Bioinformatica/reduced_dataset_100k.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# 1. Selezioniamo le colonne rilevanti\n",
    "paziente_diagnosi_malattia = dataset[['CODICE_FISCALE_ASSISTITO', 'ICD9_CM', 'DATA_PRESCRIZIONE', 'ANNO_NASCITA', 'SESSO', 'DISEASE_LABEL']]\n",
    "print(dataset['DATA_PRESCRIZIONE'])\n",
    "# Convertiamo le date in formato datetime\n",
    "paziente_diagnosi_malattia['DATA_PRESCRIZIONE'] = pd.to_datetime(paziente_diagnosi_malattia['DATA_PRESCRIZIONE'])\n",
    "import pandas as pd\n",
    "\n",
    "# Supponiamo che 'dataset' sia il DataFrame originale\n",
    "paziente_diagnosi_malattia = dataset[['CODICE_FISCALE_ASSISTITO', 'ICD9_CM', 'DATA_PRESCRIZIONE', 'ANNO_NASCITA', 'SESSO', 'DISEASE_LABEL']]\n",
    "\n",
    "# 1. Converte la colonna DATA_PRESCRIZIONE in datetime\n",
    "paziente_diagnosi_malattia['DATA_PRESCRIZIONE'] = pd.to_datetime(paziente_diagnosi_malattia['DATA_PRESCRIZIONE'])\n",
    "\n",
    "# 2. Ordina il dataset in ordine crescente in base alla DATA_PRESCRIZIONE\n",
    "paziente_diagnosi_malattia = paziente_diagnosi_malattia.sort_values(by='DATA_PRESCRIZIONE')\n",
    "\n",
    "#Rimozione duplicati\n",
    "#paziente_diagnosi_malattia = paziente_diagnosi_malattia.groupby(['CODICE_FISCALE_ASSISTITO', 'ICD9_CM']).first().reset_index()\n",
    "# 3. Definisce le soglie temporali per lo split:\n",
    "#    - Training: eventi antecedenti al 1° gennaio 2019\n",
    "#    - Validation: eventi dal 1° gennaio 2019 fino al 31 dicembre 2020\n",
    "#    - Test: eventi dal 1° gennaio 2021 in poi\n",
    "# Modifica originale problematica:\n",
    "# train_end = pd.Timestamp('2013-01-01')\n",
    "# val_end = pd.Timestamp('2016-01-01')\n",
    "\n",
    "def create_temporal_splits(df, patient_id_col='CODICE_FISCALE_ASSISTITO'):\n",
    "    # Calcola ultima data per ogni paziente\n",
    "    patient_last_date = df.groupby(patient_id_col)['DATA_PRESCRIZIONE'].max().reset_index()\n",
    "    \n",
    "    # Definisci split temporali coerenti\n",
    "    train_cutoff = pd.Timestamp('2018-05-16')  # 70% timeline\n",
    "    val_cutoff = pd.Timestamp('2018-05-31')     # 20% \n",
    "    \n",
    "    # Assegna pazienti agli split\n",
    "    patient_last_date['split'] = 'test'\n",
    "    patient_last_date.loc[patient_last_date['DATA_PRESCRIZIONE'] <= train_cutoff, 'split'] = 'train'\n",
    "    patient_last_date.loc[(patient_last_date['DATA_PRESCRIZIONE'] > train_cutoff) & \n",
    "                         (patient_last_date['DATA_PRESCRIZIONE'] <= val_cutoff), 'split'] = 'val'\n",
    "    \n",
    "    # Merge con dati originali\n",
    "    df_split = pd.merge(df, patient_last_date[[patient_id_col, 'split']], on=patient_id_col)\n",
    "    \n",
    "    return {\n",
    "        'train': df_split[df_split['split'] == 'train'].drop(columns=['split']),\n",
    "        'val': df_split[df_split['split'] == 'val'].drop(columns=['split']),\n",
    "        'test': df_split[df_split['split'] == 'test'].drop(columns=['split'])\n",
    "    }\n",
    "\n",
    "# Prima dello split, crea un encoder persistente\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Modifica la creazione del disease_map\n",
    "disease_encoder = LabelEncoder()\n",
    "disease_encoder.fit(paziente_diagnosi_malattia['ICD9_CM'])\n",
    "\n",
    "# Salva l'encoder per inference futura\n",
    "import joblib\n",
    "joblib.dump(disease_encoder, 'disease_encoder.pkl')\n",
    "\n",
    "# Utilizza l'encoder per tutti gli split\n",
    "splits = create_temporal_splits(paziente_diagnosi_malattia)\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    splits[split_name]['ICD9_encoded'] = disease_encoder.transform(splits[split_name]['ICD9_CM'])\n",
    "train_data, val_data, test_data = splits['train'], splits['val'], splits['test']\n",
    "# Stampa il numero di eventi in ciascun set per verifica\n",
    "print(\"Training set:\", train_data.shape[0], \"eventi\")\n",
    "print(\"Validation set:\", val_data.shape[0], \"eventi\")\n",
    "print(\"Test set:\", test_data.shape[0], \"eventi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_inductiive_splits(df, patient_id_col='CODICE_FISCALE_ASSISTITO', train_frac=0.7, val_frac=0.2):\n",
    "    # pazienti unici\n",
    "    unique_patients = df[patient_id_col].unique()\n",
    "    \n",
    "    # Shuffle e split pazienti in train/test/eval\n",
    "    train_size = int(len(unique_patients) * train_frac)\n",
    "    val_size = int(len(unique_patients) * val_frac)\n",
    "    \n",
    "    train_patients = unique_patients[:train_size]\n",
    "    val_patients = unique_patients[train_size:train_size + val_size]\n",
    "    test_patients = unique_patients[train_size + val_size:]\n",
    "    \n",
    "    # Dai le etichette di split a ogni paziente\n",
    "    patient_split = pd.DataFrame({\n",
    "        patient_id_col: unique_patients,\n",
    "        'split': ['train'] * len(train_patients) + ['val'] * len(val_patients) + ['test'] * len(test_patients)\n",
    "    })\n",
    "    \n",
    "    # Merge con il dataset originale\n",
    "    df_split = pd.merge(df, patient_split, on=patient_id_col)\n",
    "    \n",
    "    return {\n",
    "        'train': df_split[df_split['split'] == 'train'].drop(columns=['split']),\n",
    "        'val': df_split[df_split['split'] == 'val'].drop(columns=['split']),\n",
    "        'test': df_split[df_split['split'] == 'test'].drop(columns=['split'])\n",
    "    }\n",
    "def create_relatiive_temporal_splits(df, patient_id_col='CODICE_FISCALE_ASSISTITO', date_col='DATA_PRESCRIZIONE'):\n",
    "    input_data = []\n",
    "    target_data = []\n",
    "    \n",
    "    for patient_id, group in df.groupby(patient_id_col):\n",
    "        # Sort by date\n",
    "        group = group.sort_values(by=date_col)\n",
    "        \n",
    "        # Split into input (first 50%) and target (remaining 50%)\n",
    "        split_index = len(group) // 2\n",
    "        input_data.append(group.iloc[:split_index])\n",
    "        target_data.append(group.iloc[split_index:])\n",
    "    \n",
    "    # Concatenate all input and target data\n",
    "    input_df = pd.concat(input_data)\n",
    "    target_df = pd.concat(target_data)\n",
    "    \n",
    "    return input_df, target_df\n",
    "\n",
    "# Load or create your dataset\n",
    "# Example: paziente_diagnosi_malattia = pd.read_csv('your_dataset.csv')\n",
    "paziente_diagnosi_malattia = paziente_diagnosi_malattia.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Step 1: Create Inductive Learning splits\n",
    "splits = create_inductiive_splits(paziente_diagnosi_malattia)\n",
    "\n",
    "# Step 2: Create Relative Temporal Splits for each split\n",
    "train_input, train_target = create_relatiive_temporal_splits(splits['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average future window length: 2.36\n",
      "Average future window length: 2.38\n",
      "First 5 input sequences:\n",
      "['414.9', '233']\n",
      "['414.9', '233', '414.9', '414.9', '414.9']\n",
      "['414.9', '233', '414.9', '414.9', '414.9', '414.9', '535.11', '414.9']\n",
      "['414.9', '233', '414.9', '414.9', '414.9', '414.9', '535.11', '414.9', '414.9', '459.9', '459.9']\n",
      "['414.9', '233', '414.9', '414.9', '414.9', '414.9', '535.11', '414.9', '414.9', '459.9', '459.9', '535.11']\n",
      "\n",
      "First 5 target sequences:\n",
      "['414.9', '414.9', '414.9']\n",
      "['414.9', '535.11', '414.9']\n",
      "['414.9', '459.9', '459.9']\n",
      "['535.11']\n",
      "['307.4']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "def create_inductive_splits(df, patient_id_col='CODICE_FISCALE_ASSISTITO', train_frac=0.6, val_frac=0.2):\n",
    "    # Get unique patients and shuffle them\n",
    "    unique_patients = df[patient_id_col].unique()\n",
    "    np.random.shuffle(unique_patients)  # Shuffle the patients\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    train_size = int(len(unique_patients) * train_frac)\n",
    "    val_size = int(len(unique_patients) * val_frac)\n",
    "    \n",
    "    train_patients = unique_patients[:train_size]\n",
    "    val_patients = unique_patients[train_size:train_size + val_size]\n",
    "    test_patients = unique_patients[train_size + val_size:]\n",
    "    \n",
    "    # Create split labels\n",
    "    patient_split = pd.DataFrame({\n",
    "        patient_id_col: unique_patients,\n",
    "        'split': ['train'] * train_size + ['val'] * val_size + ['test'] * len(test_patients)\n",
    "    })\n",
    "    \n",
    "    # Merge with the original dataframe\n",
    "    df_split = pd.merge(df, patient_split, on=patient_id_col)\n",
    "    \n",
    "    return {\n",
    "        'train': df_split[df_split['split'] == 'train'].drop(columns=['split']),\n",
    "        'val': df_split[df_split['split'] == 'val'].drop(columns=['split']),\n",
    "        'test': df_split[df_split['split'] == 'test'].drop(columns=['split'])\n",
    "    }\n",
    "\n",
    "def create_relative_temporal_splits(df, patient_id_col='CODICE_FISCALE_ASSISTITO', date_col='DATA_PRESCRIZIONE'):\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "    patient_ids = []  # Track patient IDs for each sequence\n",
    "    target_lengths = []\n",
    "    \n",
    "    grouped = df.groupby(patient_id_col)\n",
    "    \n",
    "    for patient_id, group in grouped:\n",
    "        group = group.sort_values(by=date_col)\n",
    "        date_groups = group.groupby(date_col)\n",
    "        dates = sorted(date_groups.groups.keys())\n",
    "        \n",
    "        if len(dates) < 2:\n",
    "            continue\n",
    "        \n",
    "        dates_codes = []\n",
    "        for date in dates:\n",
    "            codes = date_groups.get_group(date)['ICD9_CM'].tolist()\n",
    "            dates_codes.append(codes)\n",
    "        \n",
    "        for i in range(len(dates_codes) - 1):\n",
    "            input_sequence = sum(dates_codes[:i+1], [])\n",
    "            target_sequence = dates_codes[i+1]\n",
    "            \n",
    "            input_sequences.append(input_sequence)\n",
    "            target_sequences.append(target_sequence)\n",
    "            patient_ids.append(patient_id)  # Append patient ID\n",
    "            target_lengths.append(len(target_sequence))\n",
    "    \n",
    "    if target_lengths:\n",
    "        avg_length = sum(target_lengths) / len(target_lengths)\n",
    "        print(f\"Average future window length: {avg_length:.2f}\")\n",
    "    else:\n",
    "        print(\"No valid target sequences found.\")\n",
    "    \n",
    "    return input_sequences, target_sequences, patient_ids  # Return patient IDs\n",
    "def create_enhanced_temporal_splits(df, disease_encoder, patient_id_col='CODICE_FISCALE_ASSISTITO', date_col='DATA_PRESCRIZIONE'):\n",
    "    \"\"\"Returns input sequences, encoded future targets, patient IDs\"\"\"\n",
    "    input_sequences = []\n",
    "    all_future_targets = []\n",
    "    patient_ids = []\n",
    "    \n",
    "    grouped = df.groupby(patient_id_col)\n",
    "    \n",
    "    for pid, group in grouped:\n",
    "        group = group.sort_values(date_col)\n",
    "        date_groups = group.groupby(date_col)\n",
    "        dates = sorted(date_groups.groups.keys())\n",
    "        \n",
    "        if len(dates) < 2: continue\n",
    "        \n",
    "        # Collect codes per date (already encoded)\n",
    "        timeline = []\n",
    "        for date in dates:\n",
    "            codes = date_groups.get_group(date)['ICD9_encoded'].tolist()  # Use encoded values\n",
    "            timeline.append(codes)\n",
    "        \n",
    "        # For each time point, store input and ALL future targets\n",
    "        for i in range(len(timeline)-1):\n",
    "            input_sequences.append(sum(timeline[:i+1], []))  # Already encoded\n",
    "            all_future_targets.append(sum(timeline[i+1:], []))  # Encoded targets\n",
    "            patient_ids.append(pid)\n",
    "            \n",
    "    return input_sequences, all_future_targets, patient_ids\n",
    "\n",
    "# Fit the encoder on all unique ICD9 codes in the dataset first\n",
    "all_icd9_codes = paziente_diagnosi_malattia['ICD9_CM'].unique()\n",
    "disease_encoder = LabelEncoder()\n",
    "disease_encoder.fit(all_icd9_codes)\n",
    "\n",
    "# After creating inductive splits\n",
    "splits = create_inductive_splits(paziente_diagnosi_malattia)\n",
    "\n",
    "# Encode each split's ICD9 codes using the pre-fitted encoder\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    splits[split_name]['ICD9_encoded'] = disease_encoder.transform(splits[split_name]['ICD9_CM'])\n",
    "\n",
    "# Generate input/target sequences for each split\n",
    "train_input_seqs, train_target_seqs,  train_patient_ids = create_relative_temporal_splits(splits['train'])\n",
    "# After creating splits:\n",
    "val_inputs, val_all_future_targets, val_patient_ids = create_enhanced_temporal_splits(splits['val'], disease_encoder=disease_encoder)\n",
    "test_input_seqs, test_target_seqs,test_patient_ids = create_relative_temporal_splits(splits['test'])\n",
    "print(\"First 5 input sequences:\")\n",
    "for i in range(min(5, len(train_input_seqs))):  # Handle cases where there are fewer than 5 sequences\n",
    "    print(train_input_seqs[i])\n",
    "\n",
    "print(\"\\nFirst 5 target sequences:\")\n",
    "for i in range(min(5, len(train_target_seqs))):\n",
    "    print(train_target_seqs[i])\n",
    "# Vectorized encoding function\n",
    "def vectorized_encode_sequences(sequences, encoder):\n",
    "    # Apply the encoder to each code in each sequence\n",
    "    encoded_sequences = []\n",
    "    for seq in sequences:\n",
    "        # Convert the sequence to an array and encode it\n",
    "        encoded_seq = encoder.transform(seq)\n",
    "        encoded_sequences.append(encoded_seq)\n",
    "    return encoded_sequences\n",
    "\n",
    "# Convert to multi-hot vectors\n",
    "def create_multi_hot(target_lists, num_diseases):\n",
    "    vectors = torch.zeros(len(target_lists), num_diseases, dtype=torch.float)\n",
    "    for i, targets in enumerate(target_lists):\n",
    "        # Convert to tensor first if needed\n",
    "        unique = torch.unique(torch.tensor(targets))\n",
    "        vectors[i, unique.long()] = 1  # Ensure long tensor type\n",
    "    return vectors\n",
    "\n",
    "val_targets_multi_hot = create_multi_hot(val_all_future_targets, num_diseases=len(disease_encoder.classes_))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Encode input and target sequences using vectorized function\n",
    "train_input_encoded = vectorized_encode_sequences(train_input_seqs, disease_encoder)\n",
    "train_target_encoded = vectorized_encode_sequences(train_target_seqs, disease_encoder)\n",
    "#val_input_encoded = vectorized_encode_sequences(val_inputs_seqs, disease_encoder)\n",
    "#val_target_encoded = vectorized_encode_sequences(val_all_future_targets, disease_encoder)\n",
    "test_input_encoded = vectorized_encode_sequences(test_input_seqs, disease_encoder)\n",
    "test_target_encoded = vectorized_encode_sequences(test_target_seqs, disease_encoder)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "patient_all_targets = defaultdict(set)\n",
    "for pid, targets in zip(train_patient_ids, train_target_encoded):\n",
    "    patient_all_targets[pid].update(targets)\n",
    "\n",
    "# Update allowed targets with validation data too:\n",
    "for pid, targets in zip(val_patient_ids, val_all_future_targets):\n",
    "    patient_all_targets[pid].update(targets)\n",
    "\n",
    "\n",
    "\n",
    "for pid, targets in zip(test_patient_ids, test_target_encoded):\n",
    "    patient_all_targets[pid].update(targets)\n",
    "\n",
    "# Precompute allowed indices for each patient\n",
    "num_diseases = len(disease_encoder.classes_)\n",
    "patient_allowed_indices = {}\n",
    "\n",
    "for pid, diseases in patient_all_targets.items():\n",
    "    excluded = torch.tensor(list(diseases), dtype=torch.long)\n",
    "    allowed_mask = torch.ones(num_diseases, dtype=torch.bool)\n",
    "    allowed_mask[excluded] = False\n",
    "    allowed_indices = allowed_mask.nonzero().squeeze()\n",
    "    patient_allowed_indices[pid] = allowed_indices\n",
    "\n",
    "\n",
    "# Convert to tensors efficiently\n",
    "def sequences_to_tensors(sequences):\n",
    "    return [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "\n",
    "patient_inputs_train = sequences_to_tensors(train_input_encoded)\n",
    "patient_targets_train = sequences_to_tensors(train_target_encoded)\n",
    "#patient_inputs_val = sequences_to_tensors(val_input_encoded)\n",
    "#patient_targets_val = sequences_to_tensors(val_target_encoded)\n",
    "patient_inputs_test = sequences_to_tensors(test_input_encoded)\n",
    "patient_targets_test = sequences_to_tensors(test_target_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ComorbidityPredictor(nn.Module):\n",
    "    def __init__(self, num_diseases, emb_dim, num_gat_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # Embedding malattie globali\n",
    "        self.disease_emb = nn.Embedding(num_diseases, emb_dim)\n",
    "        \n",
    "        # GNN per il grafo comorbidità\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        for _ in range(num_gat_layers):\n",
    "            self.gnn_layers.append(GATv2Conv(emb_dim, emb_dim, edge_dim=1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # encoder temporale per lo storico del paziente\n",
    "        self.rnn = nn.GRU(emb_dim, emb_dim, batch_first=True)\n",
    "        \n",
    "        # testa di predizione\n",
    "        self.classifier = nn.Linear(emb_dim, num_diseases)\n",
    "\n",
    "    \n",
    "    def forward(self, patient_sequences, global_graph):\n",
    "        #Patient_sequences è la prima metà, global graph è la comorbidità di tutte le prime metà dello storico dei paazienti\n",
    "        # Step 1: aggiorna i disease embedding con il global graph\n",
    "        x = self.disease_emb.weight\n",
    "        for gnn_layer in self.gnn_layers:\n",
    "            x = F.relu(gnn_layer(x, global_graph.edge_index,edge_attr=global_graph.edge_attr))\n",
    "            x = self.dropout(x)\n",
    "        global_embs = x\n",
    "        \n",
    "        # Step 2: codifica lo storico del paziente\n",
    "        seq_embs = global_embs[patient_sequences]  # lookup per gli embedding delle malattie\n",
    "        outputs, h_n = self.rnn(seq_embs)  # h_n: [1, batch_size, emb_dim]\n",
    "       \n",
    "        # Step 3: predici le malattie future\n",
    "        logits = self.classifier(h_n.squeeze(0))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "def build_graph(train_input, icd9_encoded_col='ICD9_encoded'):\n",
    "    \"\"\"\n",
    "    Builds a comorbidity graph with Jaccard Index as edge weights.\n",
    "    \n",
    "    Args:\n",
    "        train_input (pd.DataFrame): Training input data with encoded ICD9 codes.\n",
    "        icd9_encoded_col (str): Column name for encoded ICD9 codes.\n",
    "    \n",
    "    Returns:\n",
    "        torch_geometric.data.Data: Graph with edges weighted by Jaccard Index.\n",
    "    \"\"\"\n",
    "    co_occurrence_counts = defaultdict(int)\n",
    "    disease_counts = defaultdict(int)\n",
    "    \n",
    "    # Process each patient's unique diseases\n",
    "    for _, group in train_input.groupby('CODICE_FISCALE_ASSISTITO'):\n",
    "        diseases = list(set(group[icd9_encoded_col].tolist()))  # Deduplicate\n",
    "        \n",
    "        # Count individual disease occurrences\n",
    "        for d in diseases:\n",
    "            disease_counts[d] += 1\n",
    "            \n",
    "        # Generate co-occurrence pairs\n",
    "        for i in range(len(diseases)):\n",
    "            for j in range(i + 1, len(diseases)):\n",
    "                pair = tuple(sorted((diseases[i], diseases[j])))\n",
    "                co_occurrence_counts[pair] += 1\n",
    "    \n",
    "    # Build edges with Jaccard Index weights\n",
    "    edge_index = []\n",
    "    edge_weights = []\n",
    "    \n",
    "    for (src, dst), count in co_occurrence_counts.items():\n",
    "        edge_index.append([src, dst])\n",
    "        n_i = disease_counts[src]\n",
    "        n_j = disease_counts[dst]\n",
    "        jaccard = count / (n_i + n_j - count)  # Jaccard Index\n",
    "        edge_weights.append(jaccard)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float)\n",
    "    \n",
    "    num_nodes = train_input[icd9_encoded_col].nunique()\n",
    "    return Data(edge_index=edge_index, edge_attr=edge_weights, num_nodes=num_nodes)\n",
    "def build_neighbor_dict(graph):\n",
    "    \"\"\"\n",
    "    Builds a dictionary mapping each disease to its cooccurring neighbors and their count.\n",
    "\n",
    "    Args:\n",
    "        graph (torch_geometric.data.Data): Comorbidity graph.\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "        dict: {disease: [(neighbor, cooccurrence count), ...]} sorted by descending count.\n",
    "    \"\"\"\n",
    "    neighbor_dict = defaultdict(list)\n",
    "    edge_index = graph.edge_index.t().tolist()  # Shape [num_edges, 2]\n",
    "    edge_weights = graph.edge_attr.tolist()\n",
    "    \n",
    "    for (u, v), w in zip(edge_index, edge_weights):\n",
    "        neighbor_dict[u].append((v, w))\n",
    "        neighbor_dict[v].append((u, w))\n",
    "    \n",
    "    # Sort neighbors by co-occurrence count (descending)\n",
    "    for disease in neighbor_dict:\n",
    "        neighbor_dict[disease].sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return neighbor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bpr_loss(pos_scores, neg_scores):  # Add a margin parameter\n",
    "    loss = -torch.log(torch.sigmoid(pos_scores - neg_scores)).mean() # Subtract margin\n",
    "    return loss\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def evaluate_loss(patient_inputs_val, patient_targets_val, model, global_graph, \n",
    "                  num_diseases=len(paziente_diagnosi_malattia['ICD9_CM'].unique()), batch_size=32):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        indices = list(range(len(patient_inputs_val)))\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            \n",
    "            # Get and pad input sequences\n",
    "            batch_inputs = [patient_inputs_val[idx] for idx in batch_indices]\n",
    "            batch_inputs_padded = nn.utils.rnn.pad_sequence(\n",
    "                [torch.tensor(seq) for seq in batch_inputs], \n",
    "                batch_first=True, \n",
    "                padding_value=0\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(batch_inputs_padded, global_graph)\n",
    "            \n",
    "            # Create multi-hot targets (for cumulative evaluation)\n",
    "            batch_target_vectors = torch.zeros(len(batch_indices), num_diseases, dtype=torch.float)\n",
    "            for b, idx in enumerate(batch_indices):\n",
    "                target_seq = patient_targets_val[idx]\n",
    "                unique_diseases = torch.unique(torch.tensor(target_seq, dtype=torch.long))\n",
    "\n",
    "                batch_target_vectors[b, unique_diseases] = 1\n",
    "            \n",
    "            # Retrieve the corresponding patient IDs for this batch\n",
    "            batch_patient_ids = [val_patient_ids[idx] for idx in batch_indices]\n",
    "            \n",
    "            # Use batch_patient_ids to get allowed indices (or for any other patient-specific logic)\n",
    "            batch_allowed_indices = [patient_allowed_indices[pid] for pid in batch_patient_ids]\n",
    "            \n",
    "            pos_scores, neg_scores = sample_pos_neg_pairs(\n",
    "                logits, batch_target_vectors, num_diseases, neighbor_dict,\n",
    "                use_hard=False,  # Evaluation: no hard negatives\n",
    "                batch_allowed_indices=batch_allowed_indices\n",
    "            )\n",
    "            \n",
    "            loss = bpr_loss(pos_scores, neg_scores)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / (len(indices) / batch_size)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "global_graph = build_graph(splits['train'])\n",
    "\n",
    "neighbor_dict = build_neighbor_dict(global_graph)\n",
    "\n",
    "\n",
    "def sample_pos_neg_pairs(logits, targets, num_diseases, neighbor_dict, use_hard, batch_allowed_indices):\n",
    "    \"\"\"\n",
    "    Samples positive and hard negative disease pairs using comorbidity graph.\n",
    "    \n",
    "    Args:\n",
    "        logits (torch.Tensor): Model predictions of shape [batch_size, num_diseases].\n",
    "        targets (torch.Tensor): Ground truth multi-hot vectors of shape [batch_size, num_diseases].\n",
    "        num_diseases (int): Total number of diseases.\n",
    "        neighbor_dict (dict): Precomputed neighbor relationships from the comorbidity graph.\n",
    "    \n",
    "    Returns:\n",
    "        pos_scores (torch.Tensor): Scores for positive samples.\n",
    "        neg_scores (torch.Tensor): Scores for negative samples.\n",
    "    \"\"\"\n",
    "    if use_hard == False:\n",
    "        batch_size = logits.size(0)\n",
    "        pos_scores = []\n",
    "        neg_scores = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            pos_diseases = torch.nonzero(targets[b]).squeeze(-1)\n",
    "            allowed_indices = batch_allowed_indices[b]\n",
    "            \n",
    "            for pos_disease in pos_diseases:\n",
    "                pos_score = logits[b, pos_disease]\n",
    "                \n",
    "                \n",
    "                num_neg_samples = 1\n",
    "                if len(allowed_indices) == 0:\n",
    "                    neg_diseases = torch.randint(0, num_diseases, (num_neg_samples,))\n",
    "                else:\n",
    "                    rand_indices = torch.randint(0, len(allowed_indices), (num_neg_samples,))\n",
    "                    neg_diseases = allowed_indices[rand_indices]\n",
    "                \n",
    "                neg_scores_batch = logits[b, neg_diseases]\n",
    "                mean_neg_score = neg_scores_batch.mean()\n",
    "                \n",
    "                pos_scores.append(pos_score)\n",
    "                neg_scores.append(mean_neg_score)\n",
    "        return torch.stack(pos_scores), torch.stack(neg_scores)\n",
    "    else:\n",
    "        # Archi negativi difficili\n",
    "        pass\n",
    "        batch_size = logits.size(0)\n",
    "        pos_scores = []\n",
    "        neg_scores = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Convert positive diseases to a list of integers for quick lookup\n",
    "            pos_diseases_tensor = torch.nonzero(targets[b]).squeeze(-1)\n",
    "            pos_diseases_list = pos_diseases_tensor.tolist()\n",
    "            \n",
    "            for pos_disease_tensor in pos_diseases_tensor:\n",
    "                pos_disease = pos_disease_tensor.item()\n",
    "                pos_score = logits[b, pos_disease]\n",
    "                \n",
    "                #ottieni i negativi difficili dal grafo di comorbidità\n",
    "                candidates = neighbor_dict.get(pos_disease, [])\n",
    "                # Filtra le malattie già presenti \n",
    "                valid_candidates = [(n, w) for (n, w) in candidates if n not in pos_diseases_list]\n",
    "                \n",
    "                if valid_candidates:\n",
    "                    #estrai gli indici e i pesi\n",
    "                    neg_indices, neg_weights = zip(*valid_candidates)\n",
    "                    weights_tensor = torch.tensor(neg_weights, dtype=torch.float)\n",
    "                    \n",
    "                    #normalizza i pesi in probabilità e fai il sampling\n",
    "                    probs = weights_tensor / weights_tensor.sum()\n",
    "                    sampled_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "                    neg_disease = neg_indices[sampled_idx]\n",
    "                else:\n",
    "                    # Fallback a random sampling se non ci sono candidati validi\n",
    "                    neg_disease = torch.randint(0, num_diseases, (1,)).item()\n",
    "                    while neg_disease in pos_diseases_list:\n",
    "                        neg_disease = torch.randint(0, num_diseases, (1,)).item()\n",
    "                \n",
    "                neg_score = logits[b, neg_disease]\n",
    "                pos_scores.append(pos_score)\n",
    "                neg_scores.append(neg_score)\n",
    "        \n",
    "        pos_scores = torch.stack(pos_scores) if pos_scores else torch.tensor([])\n",
    "        neg_scores = torch.stack(neg_scores) if neg_scores else torch.tensor([])\n",
    "        return pos_scores, neg_scores\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait before stopping if no improvement.\n",
    "            delta (float): Minimum change in monitored metric to qualify as improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss  # Higher score means lower loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class PatientDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets, num_diseases):\n",
    "        self.inputs = inputs\n",
    "        self.num_diseases = num_diseases\n",
    "        # Precompute multi-hot target vectors\n",
    "        self.targets = [self._to_multi_hot(t) for t in targets]\n",
    "\n",
    "    def _to_multi_hot(self, target_seq):\n",
    "        vec = torch.zeros(self.num_diseases, dtype=torch.float)\n",
    "        unique = torch.unique(target_seq)\n",
    "        vec[unique] = 1\n",
    "        return vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    # Pad input sequences\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        inputs, batch_first=True, padding_value=0\n",
    "    )\n",
    "    targets = torch.stack(targets)\n",
    "    return inputs_padded, targets\n",
    "\n",
    "\n",
    "\n",
    "def train_module(model,num_epochs=10, patient_id_col='CODICE_FISCALE_ASSISTITO',icd9_encoded_col = 'ICD9_encoded',num_diseases=len(paziente_diagnosi_malattia['ICD9_CM'].unique()),batch_size=100, lr=0.001,patience=5):  # Pseudocode: Training loop\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "  \n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        indices = torch.randperm(len(patient_inputs_train))\n",
    "        print(len(indices))\n",
    "\n",
    "        # Calculate 10% of the list length\n",
    "\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            batch_inputs = [patient_inputs_train[idx] for idx in batch_indices]\n",
    "            batch_targets = [patient_targets_train[idx] for idx in batch_indices]\n",
    "            batch_patient_ids = [train_patient_ids[idx] for idx in batch_indices]  # Retrieve patient IDs\n",
    "            \n",
    "\n",
    "            # Get allowed indices for each patient in the batch\n",
    "            batch_allowed_indices = [patient_allowed_indices[pid] for pid in batch_patient_ids]\n",
    "            \n",
    "            # Pad input sequences\n",
    "            batch_inputs = nn.utils.rnn.pad_sequence(batch_inputs, batch_first=True, padding_value=0)\n",
    "            batch_size_current, seq_len = batch_inputs.size()\n",
    "            \n",
    "            # Create multi-hot target vectors\n",
    "            batch_target_vectors = torch.zeros(batch_size_current, num_diseases, dtype=torch.float)\n",
    "            for b, target_seq in enumerate(batch_targets):\n",
    "                unique_diseases = torch.unique(target_seq)\n",
    "                batch_target_vectors[b, unique_diseases] = 1\n",
    "            \n",
    "            # Forward pass and loss calculation\n",
    "            use_hard = random.random() < 1/10\n",
    "            logits = model(batch_inputs, global_graph)\n",
    "            pos_scores, neg_scores = sample_pos_neg_pairs(\n",
    "                    logits, batch_target_vectors, num_diseases, neighbor_dict, \n",
    "                    use_hard=use_hard, batch_allowed_indices=batch_allowed_indices\n",
    "                )\n",
    "                \n",
    "            # Ensure batch_targets has shape [batch_size]\n",
    "            loss = bpr_loss(pos_scores, neg_scores)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            print(loss.item())\n",
    "            num_batches += 1\n",
    "        model.eval()\n",
    "        val_loss = evaluate_loss(val_inputs, val_targets_multi_hot, model, global_graph)\n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        training_losses.append(avg_train_loss)\n",
    "        validation_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_train_loss}\")\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    return training_losses, validation_losses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(preds, targets, k=100):\n",
    "    batch_size = preds.size(0)\n",
    "    _, top_k_indices = torch.topk(preds, k, dim=-1)  # Top-k predictions\n",
    "    recall_scores = []\n",
    "    \n",
    "    total_tps = 0  # Total true positives across the batch\n",
    "    total_fps = 0  # Total false positives across the batch\n",
    "    total_fns = 0  # Total false negatives across the batch\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        true_positives = torch.sum(targets[b, top_k_indices[b]]).item()\n",
    "        total_positives = torch.sum(targets[b]).item()\n",
    "        false_negatives = total_positives - true_positives\n",
    "        false_positives = k - true_positives\n",
    "        \n",
    "        total_tps += true_positives\n",
    "        total_fps += false_positives\n",
    "        total_fns += false_negatives\n",
    "        \n",
    "        recall = true_positives / total_positives if total_positives > 0 else 0\n",
    "        recall_scores.append(recall)\n",
    "    \n",
    "    print(f\"Recall@{k}: True Positives: {total_tps}, False Positives: {total_fps}, False Negatives: {total_fns}\")\n",
    "    return sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "def precision_at_k(preds, targets, k=100):\n",
    "    batch_size = preds.size(0)\n",
    "    _, top_k_indices = torch.topk(preds, k, dim=-1)\n",
    "    precision_scores = []\n",
    "    for b in range(batch_size):\n",
    "        true_positives = torch.sum(targets[b, top_k_indices[b]]).item()\n",
    "        precision = true_positives / k\n",
    "        precision_scores.append(precision)\n",
    "    return sum(precision_scores) / len(precision_scores)\n",
    "\n",
    "def f1_at_k(preds, targets, k=100):\n",
    "    batch_size = preds.size(0)\n",
    "    _, top_k_indices = torch.topk(preds, k, dim=-1)\n",
    "    f1_scores = []\n",
    "    for b in range(batch_size):\n",
    "        tp = torch.sum(targets[b, top_k_indices[b]]).item()\n",
    "        total_p = torch.sum(targets[b]).item()\n",
    "        recall = tp / total_p if total_p > 0 else 0\n",
    "        precision = tp / k\n",
    "        if (precision + recall) == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_scores.append(f1)\n",
    "    return sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "def average_precision_at_k(preds, targets, k=100):\n",
    "    batch_size = preds.size(0)\n",
    "    ap_scores = []\n",
    "    for b in range(batch_size):\n",
    "        scores = preds[b]\n",
    "        target = targets[b]\n",
    "        _, top_k_indices = torch.topk(scores, k)\n",
    "        relevant = target[top_k_indices].cpu().numpy()\n",
    "        tp_cumulative = np.cumsum(relevant)\n",
    "        precision_at_positions = tp_cumulative / (np.arange(k) + 1)\n",
    "        sum_precision = np.sum(precision_at_positions * relevant)\n",
    "        total_positives = torch.sum(target).item()\n",
    "        ap = sum_precision / total_positives if total_positives > 0 else 0.0\n",
    "        ap_scores.append(ap)\n",
    "    return sum(ap_scores) / len(ap_scores)\n",
    "\n",
    "def ndcg_at_k(preds, targets, k=100):\n",
    "    batch_size = preds.size(0)\n",
    "    ndcg_scores = []\n",
    "    for b in range(batch_size):\n",
    "        scores = preds[b]\n",
    "        target = targets[b]\n",
    "        _, top_k_indices = torch.topk(scores, k)\n",
    "        relevance = target[top_k_indices].cpu().numpy()\n",
    "        dcg = 0.0\n",
    "        for i, rel in enumerate(relevance):\n",
    "            if rel:\n",
    "                dcg += 1.0 / np.log2(i + 2)  # Position starts at 1, so i+2\n",
    "        ideal_relevance = torch.sort(target, descending=True)[0][:k].cpu().numpy()\n",
    "        idcg = 0.0\n",
    "        for i, rel in enumerate(ideal_relevance):\n",
    "            if rel:\n",
    "                idcg += 1.0 / np.log2(i + 2)\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        ndcg_scores.append(ndcg)\n",
    "    return sum(ndcg_scores) / len(ndcg_scores)\n",
    "\n",
    "def strict_accuracy(preds, targets, threshold=0.5):\n",
    "    batch_size = preds.size(0)\n",
    "    num_diseases = preds.size(1)\n",
    "    \n",
    "    # Apply threshold to convert predictions into binary values\n",
    "    preds_binary = (preds > threshold).float()\n",
    "    \n",
    "    total_correct = 0  # Total number of correct predictions (TP + TN)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        correct_predictions = torch.sum((preds_binary[b] == targets[b]).float()).item()\n",
    "        total_correct += correct_predictions\n",
    "    \n",
    "    accuracy = total_correct / (batch_size * num_diseases)\n",
    "    print(f\"Strict Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "def eval_module(patient_inputs_val, patient_targets_val, model, global_graph, k=3, \n",
    "                num_diseases=len(paziente_diagnosi_malattia['ICD9_CM'].unique()), batch_size=32):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        indices = list(range(len(patient_inputs_val)))\n",
    "        \n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            \n",
    "            # Get and pad input sequences\n",
    "            batch_inputs = [patient_inputs_val[idx] for idx in batch_indices]\n",
    "            batch_inputs_padded = nn.utils.rnn.pad_sequence(\n",
    "                [torch.tensor(seq) for seq in batch_inputs], \n",
    "                batch_first=True, \n",
    "                padding_value=0\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(batch_inputs_padded, global_graph)  # Shape: [batch_size, num_diseases]\n",
    "            \n",
    "            # Create multi-hot targets\n",
    "            batch_target_vectors = torch.zeros(len(batch_indices), num_diseases, dtype=torch.float)\n",
    "            for b, idx in enumerate(batch_indices):\n",
    "                target_seq = patient_targets_val[idx]\n",
    "                unique_diseases = torch.unique(torch.tensor(target_seq))\n",
    "                batch_target_vectors[b, unique_diseases] = 1\n",
    "            \n",
    "            all_preds.append(logits)\n",
    "            all_targets.append(batch_target_vectors)\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)  # [total_samples, num_diseases]\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    \n",
    "    # Compute metrics with k=2\n",
    "\n",
    "    \n",
    "    recall = recall_at_k(all_preds, all_targets, k=k)\n",
    "    precision = precision_at_k(all_preds, all_targets, k=k)\n",
    "    f1 = f1_at_k(all_preds, all_targets, k=k)\n",
    "    ap = average_precision_at_k(all_preds, all_targets, k=k)\n",
    "    ndcg = ndcg_at_k(all_preds, all_targets, k=k)\n",
    "    accuracy = strict_accuracy(all_preds, all_targets)\n",
    "    metrics = {\n",
    "        'recall@': recall,\n",
    "        'precision@2': precision,\n",
    "        'f1@2': f1,\n",
    "        'ndcg@2': ndcg\n",
    "    }\n",
    "    print(f\"Accuracy@{k}: {accuracy:.4f}\")\n",
    "    print(f\"Recall@{k}: {recall:.4f}\")\n",
    "    print(f\"Precision@{k}: {precision:.4f}\")\n",
    "    print(f\"F1@{k}: {f1:.4f}\")\n",
    "    print(f\"NDCG@{k}: {ndcg:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def cumulative_recall(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the cumulative recall for each patient.\n",
    "    For each patient, the true illnesses are ranked by the model’s predictions.\n",
    "    Then, we count how many of the true illnesses appear in the top-N predictions,\n",
    "    where N is the total number of true illnesses for that patient.\n",
    "    \"\"\"\n",
    "    # Get prediction ranks for each sample\n",
    "    ranked = torch.argsort(predictions, dim=1, descending=True)  # shape: [batch_size, num_diseases]\n",
    "    \n",
    "    # Reorder targets based on the ranking\n",
    "    gathered = torch.gather(targets, 1, ranked)  # same shape as predictions\n",
    "    \n",
    "    # Get the indices (row, col) where there is a true label\n",
    "    nonzero = torch.nonzero(gathered)  # each row is [patient_index, rank_position]\n",
    "    \n",
    "    batch_recalls = []\n",
    "    for i in range(predictions.size(0)):\n",
    "        # Total number of true future illnesses for patient i\n",
    "        total_future = int(targets[i].sum().item())\n",
    "        if total_future == 0:\n",
    "            continue  # Skip patients with no future illnesses\n",
    "        \n",
    "        # Select positions for patient i (i.e. rows in nonzero where the first column equals i)\n",
    "        pos_i = nonzero[nonzero[:, 0] == i][:, 1]\n",
    "        \n",
    "        # Count how many true illnesses are found in the top-N predictions, where N = total_future\n",
    "        hits = (pos_i < total_future).sum().item()\n",
    "        batch_recalls.append(hits / total_future)\n",
    "    \n",
    "    # If no patients have future illnesses, return 0.0 to avoid division by zero later.\n",
    "    if len(batch_recalls) == 0:\n",
    "        return 0.0\n",
    "    return torch.tensor(batch_recalls).mean().item()\n",
    "\n",
    "\n",
    "def cumulative_precision(predictions, targets):\n",
    "    \"\"\"\n",
    "    Computes the cumulative precision for each patient.\n",
    "    For each patient, the top-N predictions (where N is the number of true illnesses)\n",
    "    are compared against the true labels to compute precision.\n",
    "    \"\"\"\n",
    "    # Get prediction ranks\n",
    "    ranked = torch.argsort(predictions, dim=1, descending=True)\n",
    "    \n",
    "    batch_precisions = []\n",
    "    for i in range(predictions.size(0)):\n",
    "        total_future = int(targets[i].sum().item())\n",
    "        if total_future == 0:\n",
    "            continue  # Skip patients with no future illnesses\n",
    "        \n",
    "        # Get top-N predictions where N equals the number of true future illnesses\n",
    "        top_preds = ranked[i, :total_future]\n",
    "        \n",
    "        # Count how many of these predictions are correct\n",
    "        correct = targets[i, top_preds].sum().item()\n",
    "        batch_precisions.append(correct / total_future)\n",
    "    \n",
    "    if len(batch_precisions) == 0:\n",
    "        return 0.0\n",
    "    return torch.tensor(batch_precisions).mean().item()\n",
    "def simulate_random_precision(targets, num_diseases=len(paziente_diagnosi_malattia['ICD9_CM'].unique()), num_simulations=100):\n",
    "    \"\"\"\n",
    "    Simulates the cumulative precision for random predictions.\n",
    "\n",
    "    Args:\n",
    "        targets (torch.Tensor): Batch of target vectors (multi-hot encoded).\n",
    "        num_diseases (int): Number of possible diseases.\n",
    "        num_simulations (int): Number of simulations to run for averaging.\n",
    "\n",
    "    Returns:\n",
    "        float: Average cumulative precision from random simulations.\n",
    "    \"\"\"\n",
    "    avg_random_precision = 0.0\n",
    "\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # Generate random predictions between 0 and 1\n",
    "        random_predictions = torch.rand(targets.size(0), num_diseases)\n",
    "\n",
    "        avg_random_precision += cumulative_precision(random_predictions, targets)\n",
    "\n",
    "    return avg_random_precision / num_simulations\n",
    "\n",
    "def enhanced_eval_module(model, patient_inputs_val, targets_multi_hot, \n",
    "                        global_graph, k=10, batch_size=32):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process in batches\n",
    "        for i in range(0, len(patient_inputs_val), batch_size):\n",
    "            batch_inputs = patient_inputs_val[i:i+batch_size]\n",
    "            \n",
    "            # Pad sequences\n",
    "            padded_batch = nn.utils.rnn.pad_sequence(\n",
    "                [torch.tensor(seq) for seq in batch_inputs],\n",
    "                batch_first=True,\n",
    "                padding_value=0\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = model(padded_batch, global_graph)\n",
    "            all_preds.append(logits)\n",
    "    \n",
    "    # Combine predictions and calculate metrics\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    \n",
    "    #null_hypotesis=simulate_random_precision(targets_multi_hot)\n",
    "    cr = cumulative_recall(all_preds, targets_multi_hot)\n",
    "    cp = cumulative_precision(all_preds,targets_multi_hot)\n",
    "    print(f\"Cumulative Recall: {cr:.3f}, Cumulative Precision: {cp:.3f}\")\n",
    "    #print(f\"Coumulative Precision Random: {null_hypotesis:.3f}\")\n",
    "\n",
    "\n",
    "        \n",
    "    # Calculate metrics using precomputed targets\n",
    "    metrics = {\n",
    "        f'recall@{k}': recall_at_k(all_preds, targets_multi_hot, k),\n",
    "        f'precision@{k}': precision_at_k(all_preds, targets_multi_hot, k),\n",
    "        f'ndcg@{k}': ndcg_at_k(all_preds, targets_multi_hot, k),\n",
    "        f'map@{k}': average_precision_at_k(all_preds, targets_multi_hot, k)\n",
    "    }\n",
    "    \n",
    "    print(f\"Cumulative Evaluation Metrics @{k}:\")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name}: {value:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelf = ComorbidityPredictor(num_diseases=len(paziente_diagnosi_malattia['ICD9_CM'].unique()), emb_dim=128)\n",
    "#modelf = torch.compile(\n",
    " #   modelf,\n",
    " #   mode='max-autotune',\n",
    " #   fullgraph=True,  # For Apple Silicon optimization\n",
    "  #  dynamic=False\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24911\n",
      "0.6970424056053162\n",
      "0.6780812740325928\n",
      "0.6648303270339966\n",
      "0.6292657852172852\n",
      "0.6162756085395813\n",
      "0.609890878200531\n",
      "0.6161750555038452\n",
      "0.6155325174331665\n",
      "0.5631561279296875\n",
      "0.5293117761611938\n",
      "0.4832388460636139\n",
      "0.46731290221214294\n",
      "0.4395473003387451\n",
      "0.4203219413757324\n",
      "0.38595637679100037\n",
      "0.40305671095848083\n",
      "0.34743914008140564\n",
      "0.34353458881378174\n",
      "0.35598158836364746\n",
      "0.3071296215057373\n",
      "0.3254344165325165\n",
      "0.26171156764030457\n",
      "0.26741838455200195\n",
      "0.28033435344696045\n",
      "0.2740263044834137\n",
      "0.26859989762306213\n",
      "0.26458802819252014\n",
      "0.44706347584724426\n",
      "0.27824264764785767\n",
      "0.19849400222301483\n",
      "0.23735953867435455\n",
      "0.30203282833099365\n",
      "0.2364504039287567\n",
      "0.1704026609659195\n",
      "0.2121313065290451\n",
      "0.2424967885017395\n",
      "0.23846417665481567\n",
      "0.38622045516967773\n",
      "0.17764118313789368\n",
      "0.2367008775472641\n",
      "0.1671903133392334\n",
      "0.18841220438480377\n",
      "0.17413200438022614\n",
      "0.2065419852733612\n",
      "0.20057129859924316\n",
      "0.2154971808195114\n",
      "0.20360369980335236\n",
      "0.1749347746372223\n",
      "0.1753130853176117\n",
      "0.22640183568000793\n",
      "0.41862791776657104\n",
      "0.16211487352848053\n",
      "0.15699365735054016\n",
      "0.19736407697200775\n",
      "0.21022559702396393\n",
      "0.17496797442436218\n",
      "0.23223218321800232\n",
      "0.17601479589939117\n",
      "0.4400618374347687\n",
      "0.22719159722328186\n",
      "0.21269746124744415\n",
      "0.16537407040596008\n",
      "0.2075064778327942\n",
      "0.14813567698001862\n",
      "0.20588913559913635\n",
      "0.15456490218639374\n",
      "0.3845333456993103\n",
      "0.19122351706027985\n",
      "0.21583083271980286\n",
      "0.23050901293754578\n",
      "0.17575646936893463\n",
      "0.14082780480384827\n",
      "0.12754099071025848\n",
      "0.14379526674747467\n",
      "0.2087228149175644\n",
      "0.11884057521820068\n",
      "0.15922434628009796\n",
      "0.1738295704126358\n",
      "0.1529848426580429\n",
      "0.17738795280456543\n",
      "0.18877089023590088\n",
      "0.2669646143913269\n",
      "0.14666026830673218\n",
      "0.1767018437385559\n",
      "0.1958480328321457\n",
      "0.1611313372850418\n",
      "0.22986361384391785\n",
      "0.13758383691310883\n",
      "0.14036868512630463\n",
      "0.2302233874797821\n",
      "0.20953691005706787\n",
      "0.14832082390785217\n",
      "0.22717441618442535\n",
      "0.11132866889238358\n",
      "0.20754747092723846\n",
      "0.11499003320932388\n",
      "0.19194245338439941\n",
      "0.24214507639408112\n",
      "0.15183459222316742\n",
      "0.37014642357826233\n",
      "0.11949743330478668\n",
      "0.20669786632061005\n",
      "0.22790874540805817\n",
      "0.18866005539894104\n",
      "0.2172153890132904\n",
      "0.17202310264110565\n",
      "0.169086292386055\n",
      "0.15785986185073853\n",
      "0.10719403624534607\n",
      "0.1694227010011673\n",
      "0.13614025712013245\n",
      "0.2698867917060852\n",
      "0.14202433824539185\n",
      "0.15652349591255188\n",
      "0.279717355966568\n",
      "0.1635800004005432\n",
      "0.15641817450523376\n",
      "0.16268666088581085\n",
      "0.16517207026481628\n",
      "0.1633887141942978\n",
      "0.20452649891376495\n",
      "0.20742182433605194\n",
      "0.10286842286586761\n",
      "0.20133090019226074\n",
      "0.1466829776763916\n",
      "0.13372118771076202\n",
      "0.11305955052375793\n",
      "0.1464216262102127\n",
      "0.3906780779361725\n",
      "0.4944484829902649\n",
      "0.12040906399488449\n",
      "0.11830756068229675\n",
      "0.16233466565608978\n",
      "0.13459713757038116\n",
      "0.12149003893136978\n",
      "0.23876503109931946\n",
      "0.12220869213342667\n",
      "0.18929360806941986\n",
      "0.1991700381040573\n",
      "0.07897348701953888\n",
      "0.20027530193328857\n",
      "0.14631575345993042\n",
      "0.4394567608833313\n",
      "0.19947585463523865\n",
      "0.13017810881137848\n",
      "0.16972026228904724\n",
      "0.20283418893814087\n",
      "0.13062609732151031\n",
      "0.14439080655574799\n",
      "0.17800308763980865\n",
      "0.09890980273485184\n",
      "0.14327022433280945\n",
      "0.17708580195903778\n",
      "0.126332625746727\n",
      "0.24251411855220795\n",
      "0.16676700115203857\n",
      "0.2370891273021698\n",
      "0.19375978410243988\n",
      "0.17571617662906647\n",
      "0.17623388767242432\n",
      "0.13319608569145203\n",
      "0.10840124636888504\n",
      "0.1553720235824585\n",
      "0.13004210591316223\n",
      "0.09547343850135803\n",
      "0.14930221438407898\n",
      "0.20951321721076965\n",
      "0.16187801957130432\n",
      "0.36258918046951294\n",
      "0.12057578563690186\n",
      "0.18794836103916168\n",
      "0.14174044132232666\n",
      "0.21880637109279633\n",
      "0.07682996988296509\n",
      "0.44480234384536743\n",
      "0.19397346675395966\n",
      "0.14520084857940674\n",
      "0.24205894768238068\n",
      "0.4518721401691437\n",
      "0.14500094950199127\n",
      "0.190000519156456\n",
      "0.18914295732975006\n",
      "0.38019654154777527\n",
      "0.1805477738380432\n",
      "0.10300009697675705\n",
      "0.19006618857383728\n",
      "0.17853301763534546\n",
      "0.186806321144104\n",
      "0.18560796976089478\n",
      "0.15194255113601685\n",
      "0.21289053559303284\n",
      "0.24996602535247803\n",
      "0.11198517680168152\n",
      "0.17312635481357574\n",
      "0.198660746216774\n",
      "0.1783483624458313\n",
      "0.1991649568080902\n",
      "0.1376199573278427\n",
      "0.16195429861545563\n",
      "0.0950995609164238\n",
      "0.12160925567150116\n",
      "0.19721627235412598\n",
      "0.1928074061870575\n",
      "0.1549113541841507\n",
      "0.22634471952915192\n",
      "0.11888280510902405\n",
      "0.1293935477733612\n",
      "0.21741603314876556\n",
      "0.22273272275924683\n",
      "0.17462080717086792\n",
      "0.20432281494140625\n",
      "0.21283069252967834\n",
      "0.17758244276046753\n",
      "0.3876423239707947\n",
      "0.24549934267997742\n",
      "0.15171244740486145\n",
      "0.15092210471630096\n",
      "0.11285008490085602\n",
      "0.18062369525432587\n",
      "0.3738774061203003\n",
      "0.19042156636714935\n",
      "0.17824102938175201\n",
      "0.19148492813110352\n",
      "0.13628952205181122\n",
      "0.18485009670257568\n",
      "0.1748400181531906\n",
      "0.20022143423557281\n",
      "0.10640391707420349\n",
      "0.1709004044532776\n",
      "0.15446312725543976\n",
      "0.20619672536849976\n",
      "0.15194430947303772\n",
      "0.1613045185804367\n",
      "0.2268899530172348\n",
      "0.18884003162384033\n",
      "0.41301193833351135\n",
      "0.1432805359363556\n",
      "0.16566279530525208\n",
      "0.25289198756217957\n",
      "0.16640543937683105\n",
      "0.1590677946805954\n",
      "0.13563193380832672\n",
      "0.13912895321846008\n",
      "0.1542661041021347\n",
      "0.16255977749824524\n",
      "0.20141538977622986\n",
      "0.19819298386573792\n",
      "0.1507158726453781\n",
      "0.17850229144096375\n",
      "0.3356739580631256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/jv2fl6j10rl9sxj0731yq5q80000gn/T/ipykernel_72400/27523296.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  unique_diseases = torch.unique(torch.tensor(target_seq, dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2210041883289814\n",
      "24911\n",
      "0.14458052814006805\n",
      "0.1426287293434143\n",
      "0.20100383460521698\n",
      "0.1974758803844452\n",
      "0.09235207736492157\n",
      "0.1792077273130417\n",
      "0.14120222628116608\n",
      "0.446838915348053\n",
      "0.1735011637210846\n",
      "0.13180850446224213\n",
      "0.446069598197937\n",
      "0.16779179871082306\n",
      "0.13242673873901367\n",
      "0.10761907696723938\n",
      "0.1248670220375061\n",
      "0.1334124356508255\n",
      "0.14388273656368256\n",
      "0.17621298134326935\n",
      "0.10325503349304199\n",
      "0.15064334869384766\n",
      "0.46825096011161804\n",
      "0.1672167330980301\n",
      "0.1977052241563797\n",
      "0.1820283830165863\n",
      "0.1592496782541275\n",
      "0.18507534265518188\n",
      "0.12048382312059402\n",
      "0.12053834646940231\n",
      "0.178767591714859\n",
      "0.1891602873802185\n",
      "0.1721513569355011\n",
      "0.1877002716064453\n",
      "0.16304275393486023\n",
      "0.20052967965602875\n",
      "0.38239628076553345\n",
      "0.13146670162677765\n",
      "0.1303347647190094\n",
      "0.13523386418819427\n",
      "0.1540929675102234\n",
      "0.4807035028934479\n",
      "0.13691987097263336\n",
      "0.2863123118877411\n",
      "0.3374609053134918\n",
      "0.20331887900829315\n",
      "0.18183718621730804\n",
      "0.14001454412937164\n",
      "0.11612675338983536\n",
      "0.1293819546699524\n",
      "0.22123201191425323\n",
      "0.1501188427209854\n",
      "0.08064267039299011\n",
      "0.14950354397296906\n",
      "0.1357778012752533\n",
      "0.15236110985279083\n",
      "0.2222994863986969\n",
      "0.1524709016084671\n",
      "0.1330156773328781\n",
      "0.21867798268795013\n",
      "0.19249889254570007\n",
      "0.21836428344249725\n",
      "0.14650648832321167\n",
      "0.35862448811531067\n",
      "0.18518483638763428\n",
      "0.15684004127979279\n",
      "0.2313086837530136\n",
      "0.11248643696308136\n",
      "0.26439955830574036\n",
      "0.19320838153362274\n",
      "0.15354371070861816\n",
      "0.4268963932991028\n",
      "0.36365753412246704\n",
      "0.09353996813297272\n",
      "0.23334752023220062\n",
      "0.17812786996364594\n",
      "0.18536920845508575\n",
      "0.3842633366584778\n",
      "0.2782599627971649\n",
      "0.19131439924240112\n",
      "0.14897975325584412\n",
      "0.176092728972435\n",
      "0.18185672163963318\n",
      "0.4378888010978699\n",
      "0.15426886081695557\n",
      "0.22159454226493835\n",
      "0.16843190789222717\n",
      "0.15377715229988098\n",
      "0.16824205219745636\n",
      "0.10545254498720169\n",
      "0.19460472464561462\n",
      "0.12273207306861877\n",
      "0.18653230369091034\n",
      "0.19699975848197937\n",
      "0.13542991876602173\n",
      "0.1032923087477684\n",
      "0.21105793118476868\n",
      "0.16297517716884613\n",
      "0.4361298680305481\n",
      "0.11905509233474731\n",
      "0.14929772913455963\n",
      "0.16361236572265625\n",
      "0.10684888064861298\n",
      "0.12488891929388046\n",
      "0.14394979178905487\n",
      "0.1331225037574768\n",
      "0.20584210753440857\n",
      "0.15830962359905243\n",
      "0.2094876915216446\n",
      "0.18692868947982788\n",
      "0.17886584997177124\n",
      "0.1623522937297821\n",
      "0.14636841416358948\n",
      "0.2200600802898407\n",
      "0.12552019953727722\n",
      "0.1103825643658638\n",
      "0.2139420360326767\n",
      "0.1545407623052597\n",
      "0.15759645402431488\n",
      "0.12140008807182312\n",
      "0.14484867453575134\n",
      "0.18551065027713776\n",
      "0.4385850131511688\n",
      "0.40248215198516846\n",
      "0.181881844997406\n",
      "0.16910038888454437\n",
      "0.1554594337940216\n",
      "0.4078950881958008\n",
      "0.17316876351833344\n",
      "0.09305282682180405\n",
      "0.1406697779893875\n",
      "0.18889741599559784\n",
      "0.3737095892429352\n",
      "0.12931354343891144\n",
      "0.166664719581604\n",
      "0.1273287832736969\n",
      "0.38157063722610474\n",
      "0.1979222595691681\n",
      "0.14579084515571594\n",
      "0.4006541669368744\n",
      "0.15732420980930328\n",
      "0.1659802794456482\n",
      "0.13665437698364258\n",
      "0.10521785914897919\n",
      "0.18843944370746613\n",
      "0.17752622067928314\n",
      "0.1847367137670517\n",
      "0.09864915162324905\n",
      "0.18558502197265625\n",
      "0.18255004286766052\n",
      "0.14287832379341125\n",
      "0.17465074360370636\n",
      "0.17778800427913666\n",
      "0.2519104480743408\n",
      "0.20083452761173248\n",
      "0.11841358244419098\n",
      "0.18489669263362885\n",
      "0.15752123296260834\n",
      "0.15097655355930328\n",
      "0.12877614796161652\n",
      "0.21856176853179932\n",
      "0.11785788834095001\n",
      "0.20874175429344177\n",
      "0.12505261600017548\n",
      "0.21044111251831055\n",
      "0.2422332912683487\n",
      "0.4371633529663086\n",
      "0.1634533703327179\n",
      "0.17315146327018738\n",
      "0.15348167717456818\n",
      "0.16479286551475525\n",
      "0.14061406254768372\n",
      "0.15892542898654938\n",
      "0.11467666178941727\n",
      "0.18590404093265533\n",
      "0.1549597829580307\n",
      "0.4229145348072052\n",
      "0.14851517975330353\n",
      "0.4343823492527008\n",
      "0.13916166126728058\n",
      "0.14895160496234894\n",
      "0.16595052182674408\n",
      "0.13149213790893555\n",
      "0.13573886454105377\n",
      "0.46659165620803833\n",
      "0.20989878475666046\n",
      "0.20879220962524414\n",
      "0.38064366579055786\n",
      "0.1703503429889679\n",
      "0.13809345662593842\n",
      "0.16003283858299255\n",
      "0.21280181407928467\n",
      "0.09437811374664307\n",
      "0.1363174170255661\n",
      "0.16540592908859253\n",
      "0.1689079999923706\n",
      "0.18056535720825195\n",
      "0.23324209451675415\n",
      "0.18796581029891968\n",
      "0.18239974975585938\n",
      "0.17985478043556213\n",
      "0.17770914733409882\n",
      "0.11845595389604568\n",
      "0.1734936386346817\n",
      "0.1285155862569809\n",
      "0.08502580970525742\n",
      "0.15970681607723236\n",
      "0.354104608297348\n",
      "0.3648451268672943\n",
      "0.16489070653915405\n",
      "0.19054199755191803\n",
      "0.1530967652797699\n",
      "0.19271156191825867\n",
      "0.12786218523979187\n",
      "0.15528066456317902\n",
      "0.34818440675735474\n",
      "0.11756018549203873\n",
      "0.151939257979393\n",
      "0.29132670164108276\n",
      "0.15364785492420197\n",
      "0.16938918828964233\n",
      "0.1901119202375412\n",
      "0.1304263025522232\n",
      "0.14695048332214355\n",
      "0.18831489980220795\n",
      "0.1505374312400818\n",
      "0.1882396936416626\n",
      "0.14937996864318848\n",
      "0.1964469999074936\n",
      "0.13938699662685394\n",
      "0.22473809123039246\n",
      "0.14155462384223938\n",
      "0.17914292216300964\n",
      "0.112765833735466\n",
      "0.19006845355033875\n",
      "0.21107369661331177\n",
      "0.16748058795928955\n",
      "0.14668311178684235\n",
      "0.11738066375255585\n",
      "0.1400074064731598\n",
      "0.15228146314620972\n",
      "0.15439744293689728\n",
      "0.18948578834533691\n",
      "0.18289335072040558\n",
      "0.12467274069786072\n",
      "0.450527548789978\n",
      "0.21708086133003235\n",
      "0.3924039602279663\n",
      "0.21695294976234436\n",
      "0.1621129959821701\n",
      "0.12978658080101013\n",
      "0.05337872728705406\n",
      "Epoch 2/10, Loss: 0.19064385356009006\n",
      "24911\n",
      "0.09958745539188385\n",
      "0.08820797502994537\n",
      "0.19821615517139435\n",
      "0.1818549782037735\n",
      "0.15338674187660217\n",
      "0.45744040608406067\n",
      "0.13673348724842072\n",
      "0.19748809933662415\n",
      "0.39885208010673523\n",
      "0.12408695369958878\n",
      "0.12571778893470764\n",
      "0.20289106667041779\n",
      "0.22709499299526215\n",
      "0.1284557431936264\n",
      "0.1636725515127182\n",
      "0.19490717351436615\n",
      "0.2012949436903\n",
      "0.17107637226581573\n",
      "0.4201411306858063\n",
      "0.14547771215438843\n",
      "0.17243339121341705\n",
      "0.16733868420124054\n",
      "0.17502136528491974\n",
      "0.1385270357131958\n",
      "0.18473058938980103\n",
      "0.12062060087919235\n",
      "0.17346584796905518\n",
      "0.19764454662799835\n",
      "0.2062355875968933\n",
      "0.13976767659187317\n",
      "0.16899539530277252\n",
      "0.15487425029277802\n",
      "0.3520340025424957\n",
      "0.3333088159561157\n",
      "0.1628161072731018\n",
      "0.38524141907691956\n",
      "0.1548900157213211\n",
      "0.19106663763523102\n",
      "0.14334380626678467\n",
      "0.12435140460729599\n",
      "0.20383311808109283\n",
      "0.19072940945625305\n",
      "0.1209682896733284\n",
      "0.2346215695142746\n",
      "0.13430598378181458\n",
      "0.1108725368976593\n",
      "0.239590123295784\n",
      "0.1407521367073059\n",
      "0.17452998459339142\n",
      "0.19009779393672943\n",
      "0.17341402173042297\n",
      "0.17793039977550507\n",
      "0.2484176903963089\n",
      "0.16460435092449188\n",
      "0.09715595841407776\n",
      "0.37230369448661804\n",
      "0.40215885639190674\n",
      "0.14838257431983948\n",
      "0.10927755385637283\n",
      "0.13883505761623383\n",
      "0.1514926254749298\n",
      "0.19346123933792114\n",
      "0.1746831238269806\n",
      "0.13697484135627747\n",
      "0.21746309101581573\n",
      "0.11938844621181488\n",
      "0.1874449998140335\n",
      "0.1223679855465889\n",
      "0.18310536444187164\n",
      "0.17404182255268097\n",
      "0.18035133183002472\n",
      "0.10364244133234024\n",
      "0.218744695186615\n",
      "0.17500732839107513\n",
      "0.38261279463768005\n",
      "0.15573041141033173\n",
      "0.14450789988040924\n",
      "0.40831199288368225\n",
      "0.1642843335866928\n",
      "0.19382067024707794\n",
      "0.22677959501743317\n",
      "0.15263985097408295\n",
      "0.19267018139362335\n",
      "0.1345069706439972\n",
      "0.16877254843711853\n",
      "0.1305171698331833\n",
      "0.2163049876689911\n",
      "0.1466570496559143\n",
      "0.14497658610343933\n",
      "0.16047389805316925\n",
      "0.18126170337200165\n",
      "0.17621712386608124\n",
      "0.16859164834022522\n",
      "0.19361530244350433\n",
      "0.4387434720993042\n",
      "0.15741242468357086\n",
      "0.36936771869659424\n",
      "0.3023794889450073\n",
      "0.099538654088974\n",
      "0.0968322679400444\n",
      "0.16315814852714539\n",
      "0.1340259611606598\n",
      "0.17762833833694458\n",
      "0.11422247439622879\n",
      "0.15134382247924805\n",
      "0.11815164238214493\n",
      "0.11908279359340668\n",
      "0.1900862604379654\n",
      "0.12932078540325165\n",
      "0.16431912779808044\n",
      "0.11611639708280563\n",
      "0.15161916613578796\n",
      "0.16342921555042267\n",
      "0.1813531070947647\n",
      "0.1701832413673401\n",
      "0.10883370786905289\n",
      "0.17198064923286438\n",
      "0.13646584749221802\n",
      "0.17464514076709747\n",
      "0.15344901382923126\n",
      "0.4001978635787964\n",
      "0.15220598876476288\n",
      "0.10657500475645065\n",
      "0.207914799451828\n",
      "0.13838808238506317\n",
      "0.12352889776229858\n",
      "0.15824928879737854\n",
      "0.16464516520500183\n",
      "0.1756652295589447\n",
      "0.17151515185832977\n",
      "0.1402795910835266\n",
      "0.23112444579601288\n",
      "0.09609866887331009\n",
      "0.4046133756637573\n",
      "0.14976942539215088\n",
      "0.22852040827274323\n",
      "0.1193050742149353\n",
      "0.11733484268188477\n",
      "0.43207868933677673\n",
      "0.11559407413005829\n",
      "0.17307645082473755\n",
      "0.13906462490558624\n",
      "0.16442450881004333\n",
      "0.21411213278770447\n",
      "0.1542179435491562\n",
      "0.1316484808921814\n",
      "0.1715579777956009\n",
      "0.1561613827943802\n",
      "0.19724829494953156\n",
      "0.1266506016254425\n",
      "0.16484221816062927\n",
      "0.20400764048099518\n",
      "0.1736999750137329\n",
      "0.10710912942886353\n",
      "0.18984675407409668\n",
      "0.12250962108373642\n",
      "0.1745380312204361\n",
      "0.15719154477119446\n",
      "0.1286320984363556\n",
      "0.15093198418617249\n",
      "0.13718773424625397\n",
      "0.3696519434452057\n",
      "0.14756546914577484\n",
      "0.15501663088798523\n",
      "0.12285291403532028\n",
      "0.15738436579704285\n",
      "0.21725785732269287\n",
      "0.12356680631637573\n",
      "0.1503407508134842\n",
      "0.15059761703014374\n",
      "0.1936967670917511\n",
      "0.16571123898029327\n",
      "0.16697357594966888\n",
      "0.15969759225845337\n",
      "0.1427573561668396\n",
      "0.14652444422245026\n",
      "0.10495610535144806\n",
      "0.15861228108406067\n",
      "0.3840961158275604\n",
      "0.13255099952220917\n",
      "0.14424234628677368\n",
      "0.2636236548423767\n",
      "0.12972676753997803\n",
      "0.19299736618995667\n",
      "0.18319076299667358\n",
      "0.17275065183639526\n",
      "0.14841492474079132\n",
      "0.19429245591163635\n",
      "0.14905980229377747\n",
      "0.18266189098358154\n",
      "0.17847003042697906\n",
      "0.1175324022769928\n",
      "0.17416971921920776\n",
      "0.23876632750034332\n",
      "0.176068514585495\n",
      "0.12156333029270172\n",
      "0.1961970329284668\n",
      "0.14441606402397156\n",
      "0.23082737624645233\n",
      "0.129230335354805\n",
      "0.1373775601387024\n",
      "0.13039259612560272\n",
      "0.1778016984462738\n",
      "0.11739780753850937\n",
      "0.13531970977783203\n",
      "0.21486937999725342\n",
      "0.15649263560771942\n",
      "0.13130268454551697\n",
      "0.15776118636131287\n",
      "0.13774916529655457\n",
      "0.15445105731487274\n",
      "0.13689200580120087\n",
      "0.16157665848731995\n",
      "0.14066152274608612\n",
      "0.1709018349647522\n",
      "0.1399485468864441\n",
      "0.12476299703121185\n",
      "0.09812415391206741\n",
      "0.16330213844776154\n",
      "0.4460168480873108\n",
      "0.3636091351509094\n",
      "0.1431335210800171\n",
      "0.18042437732219696\n",
      "0.17486535012722015\n",
      "0.14173409342765808\n",
      "0.4389474391937256\n",
      "0.13965511322021484\n",
      "0.14462769031524658\n",
      "0.19814100861549377\n",
      "0.13966894149780273\n",
      "0.20503680408000946\n",
      "0.13123197853565216\n",
      "0.14871594309806824\n",
      "0.21685728430747986\n",
      "0.40094515681266785\n",
      "0.15277265012264252\n",
      "0.4267883598804474\n",
      "0.1328619420528412\n",
      "0.1901235431432724\n",
      "0.1008821576833725\n",
      "0.15290652215480804\n",
      "0.2218538373708725\n",
      "0.11004369705915451\n",
      "0.20798993110656738\n",
      "0.16669754683971405\n",
      "0.10806771367788315\n",
      "0.21788057684898376\n",
      "0.3817805051803589\n",
      "0.1932927519083023\n",
      "0.12861160933971405\n",
      "Epoch 3/10, Loss: 0.1822188326716423\n",
      "EarlyStopping counter: 1 out of 3\n",
      "24911\n",
      "0.1275358498096466\n",
      "0.17171703279018402\n",
      "0.19738762080669403\n",
      "0.17510302364826202\n",
      "0.16915057599544525\n",
      "0.15800340473651886\n",
      "0.11332130432128906\n",
      "0.3481043577194214\n",
      "0.22045525908470154\n",
      "0.19424860179424286\n",
      "0.17515447735786438\n",
      "0.12213077396154404\n",
      "0.08079267293214798\n",
      "0.2097298502922058\n",
      "0.2041441649198532\n",
      "0.16067300736904144\n",
      "0.14084985852241516\n",
      "0.25899726152420044\n",
      "0.09420701861381531\n",
      "0.15415941178798676\n",
      "0.15701189637184143\n",
      "0.17441371083259583\n",
      "0.1431850641965866\n",
      "0.41387876868247986\n",
      "0.14367470145225525\n",
      "0.10016921162605286\n",
      "0.11624591052532196\n",
      "0.1464710235595703\n",
      "0.10577739775180817\n",
      "0.15074819326400757\n",
      "0.11447473615407944\n",
      "0.21643604338169098\n",
      "0.11332260817289352\n",
      "0.1654556691646576\n",
      "0.19026102125644684\n",
      "0.1310107707977295\n",
      "0.15571323037147522\n",
      "0.17401374876499176\n",
      "0.38711556792259216\n",
      "0.3191322684288025\n",
      "0.17854109406471252\n",
      "0.14775092899799347\n",
      "0.10660766065120697\n",
      "0.16898788511753082\n",
      "0.13783960044384003\n",
      "0.19761031866073608\n",
      "0.4022008180618286\n",
      "0.11957358568906784\n",
      "0.1624751091003418\n",
      "0.12466096878051758\n",
      "0.12982098758220673\n",
      "0.16765354573726654\n",
      "0.41584113240242004\n",
      "0.1364530324935913\n",
      "0.1617201417684555\n",
      "0.18942959606647491\n",
      "0.1431540846824646\n",
      "0.14807739853858948\n",
      "0.11709632724523544\n",
      "0.14591152966022491\n",
      "0.13519538938999176\n",
      "0.32727086544036865\n",
      "0.4565249979496002\n",
      "0.13552623987197876\n",
      "0.1968957781791687\n",
      "0.1587478220462799\n",
      "0.16837207973003387\n",
      "0.11725445836782455\n",
      "0.16882821917533875\n",
      "0.34525927901268005\n",
      "0.18113809823989868\n",
      "0.10791097581386566\n",
      "0.12547597289085388\n",
      "0.15058210492134094\n",
      "0.1504799872636795\n",
      "0.14570613205432892\n",
      "0.12402183562517166\n",
      "0.17606589198112488\n",
      "0.1702377051115036\n",
      "0.18507835268974304\n",
      "0.1691558063030243\n",
      "0.14756709337234497\n",
      "0.14351552724838257\n",
      "0.15730494260787964\n",
      "0.1663072109222412\n",
      "0.40579307079315186\n",
      "0.10750557482242584\n",
      "0.13391122221946716\n",
      "0.11499007791280746\n",
      "0.13537703454494476\n",
      "0.4186933934688568\n",
      "0.14040324091911316\n",
      "0.15440213680267334\n",
      "0.15122778713703156\n",
      "0.13845594227313995\n",
      "0.18129214644432068\n",
      "0.2646786868572235\n",
      "0.17098893225193024\n",
      "0.08098435401916504\n",
      "0.17699000239372253\n",
      "0.1736636608839035\n",
      "0.13273954391479492\n",
      "0.1605379283428192\n",
      "0.16689537465572357\n",
      "0.13968223333358765\n",
      "0.17750564217567444\n",
      "0.15561136603355408\n",
      "0.13682064414024353\n",
      "0.17367707192897797\n",
      "0.19844520092010498\n",
      "0.11716565489768982\n",
      "0.17243728041648865\n",
      "0.16024652123451233\n",
      "0.16523510217666626\n",
      "0.4160619378089905\n",
      "0.16772791743278503\n",
      "0.18907569348812103\n",
      "0.19698911905288696\n",
      "0.3424029052257538\n",
      "0.1679317206144333\n",
      "0.10194157063961029\n",
      "0.1531606912612915\n",
      "0.2215663194656372\n",
      "0.14644475281238556\n",
      "0.1541108936071396\n",
      "0.21849684417247772\n",
      "0.20787911117076874\n",
      "0.1420503854751587\n",
      "0.1308261752128601\n",
      "0.16158239543437958\n",
      "0.1251532882452011\n",
      "0.13722167909145355\n",
      "0.1761084944009781\n",
      "0.19015458226203918\n",
      "0.15111395716667175\n",
      "0.13494078814983368\n",
      "0.44276806712150574\n",
      "0.12826429307460785\n",
      "0.18365804851055145\n",
      "0.14897289872169495\n",
      "0.13289488852024078\n",
      "0.3600688576698303\n",
      "0.1510118991136551\n",
      "0.1144290417432785\n",
      "0.18065698444843292\n",
      "0.18410180509090424\n",
      "0.10239173471927643\n",
      "0.14183999598026276\n",
      "0.09665610641241074\n",
      "0.21442610025405884\n",
      "0.11753877252340317\n",
      "0.3206076920032501\n",
      "0.10642963647842407\n",
      "0.11725380271673203\n",
      "0.14866073429584503\n",
      "0.1540198177099228\n",
      "0.11925939470529556\n",
      "0.12782955169677734\n",
      "0.13491731882095337\n",
      "0.16180332005023956\n",
      "0.16511322557926178\n",
      "0.16904079914093018\n",
      "0.23920831084251404\n",
      "0.19314460456371307\n",
      "0.34476879239082336\n",
      "0.21095769107341766\n",
      "0.1781386435031891\n",
      "0.3794237971305847\n",
      "0.23485273122787476\n",
      "0.18334899842739105\n",
      "0.1276189684867859\n",
      "0.21755290031433105\n",
      "0.20883017778396606\n",
      "0.1464812308549881\n",
      "0.47224095463752747\n",
      "0.1406441628932953\n",
      "0.20519843697547913\n",
      "0.1564105749130249\n",
      "0.14034026861190796\n",
      "0.1731225699186325\n",
      "0.22743844985961914\n",
      "0.4721139073371887\n",
      "0.16898077726364136\n",
      "0.10830046236515045\n",
      "0.1661878228187561\n",
      "0.13558264076709747\n",
      "0.110636867582798\n",
      "0.17603600025177002\n",
      "0.09323164820671082\n",
      "0.16228075325489044\n",
      "0.1353941708803177\n",
      "0.15767425298690796\n",
      "0.12894268333911896\n",
      "0.10640182346105576\n",
      "0.1274077147245407\n",
      "0.11312313377857208\n",
      "0.11966075003147125\n",
      "0.1431770771741867\n",
      "0.12030134350061417\n",
      "0.15021342039108276\n",
      "0.18329840898513794\n",
      "0.13460171222686768\n",
      "0.42494434118270874\n",
      "0.16122999787330627\n",
      "0.47587549686431885\n",
      "0.136403888463974\n",
      "0.15090739727020264\n",
      "0.14189207553863525\n",
      "0.1522146612405777\n",
      "0.4032186269760132\n",
      "0.1485181599855423\n",
      "0.1615075320005417\n",
      "0.15491192042827606\n",
      "0.19793276488780975\n",
      "0.11458314955234528\n",
      "0.28190624713897705\n",
      "0.1664755493402481\n",
      "0.17303168773651123\n",
      "0.4188084304332733\n",
      "0.1625775247812271\n",
      "0.1722191423177719\n",
      "0.1287938505411148\n",
      "0.1671757698059082\n",
      "0.15068964660167694\n",
      "0.14799457788467407\n",
      "0.2108796238899231\n",
      "0.18431085348129272\n",
      "0.1341399997472763\n",
      "0.19811704754829407\n",
      "0.1700061559677124\n",
      "0.36202701926231384\n",
      "0.0921691358089447\n",
      "0.13718639314174652\n",
      "0.42523297667503357\n",
      "0.13190588355064392\n",
      "0.46326425671577454\n",
      "0.12767696380615234\n",
      "0.1433621197938919\n",
      "0.48011696338653564\n",
      "0.12411556392908096\n",
      "0.27496886253356934\n",
      "0.14556322991847992\n",
      "0.13566642999649048\n",
      "0.20745429396629333\n",
      "0.15584227442741394\n",
      "0.16778457164764404\n",
      "0.15929874777793884\n",
      "0.13749749958515167\n",
      "0.12265481054782867\n",
      "0.12454798817634583\n",
      "Epoch 4/10, Loss: 0.18309196063876151\n",
      "EarlyStopping counter: 2 out of 3\n",
      "24911\n",
      "0.14205718040466309\n",
      "0.15920096635818481\n",
      "0.12993791699409485\n",
      "0.13563673198223114\n",
      "0.1418873369693756\n",
      "0.12197216600179672\n",
      "0.16762568056583405\n",
      "0.430650532245636\n",
      "0.12786312401294708\n",
      "0.3815584182739258\n",
      "0.18139967322349548\n",
      "0.15453927218914032\n",
      "0.13123954832553864\n",
      "0.1250237077474594\n",
      "0.191584512591362\n",
      "0.08781492710113525\n",
      "0.1365005522966385\n",
      "0.18036901950836182\n",
      "0.1457253396511078\n",
      "0.28093403577804565\n",
      "0.21309059858322144\n",
      "0.16239851713180542\n",
      "0.13594043254852295\n",
      "0.18961390852928162\n",
      "0.15923763811588287\n",
      "0.11287801712751389\n",
      "0.15635937452316284\n",
      "0.15803757309913635\n",
      "0.3073529601097107\n",
      "0.09980033338069916\n",
      "0.21927152574062347\n",
      "0.1175258532166481\n",
      "0.15233157575130463\n",
      "0.2261151820421219\n",
      "0.09576082229614258\n",
      "0.1572965383529663\n",
      "0.11541224271059036\n",
      "0.12115085870027542\n",
      "0.2030903846025467\n",
      "0.13049732148647308\n",
      "0.17202605307102203\n",
      "0.11250825971364975\n",
      "0.17252831161022186\n",
      "0.1507665067911148\n",
      "0.17079700529575348\n",
      "0.18472588062286377\n",
      "0.42013195157051086\n",
      "0.13546571135520935\n",
      "0.12289931625127792\n",
      "0.14780975878238678\n",
      "0.15679512917995453\n",
      "0.1408076137304306\n",
      "0.13334941864013672\n",
      "0.08794156461954117\n",
      "0.09714021533727646\n",
      "0.2413090169429779\n",
      "0.18999965488910675\n",
      "0.1319420337677002\n",
      "0.17897775769233704\n",
      "0.1423652470111847\n",
      "0.242959663271904\n",
      "0.15711070597171783\n",
      "0.1410672515630722\n",
      "0.1594465970993042\n",
      "0.1617926061153412\n",
      "0.13020272552967072\n",
      "0.13078589737415314\n",
      "0.12215965986251831\n",
      "0.1479453593492508\n",
      "0.1680857539176941\n",
      "0.12794888019561768\n",
      "0.14260558784008026\n",
      "0.13332784175872803\n",
      "0.3381151258945465\n",
      "0.44024190306663513\n",
      "0.35044682025909424\n",
      "0.14494170248508453\n",
      "0.13295434415340424\n",
      "0.17445236444473267\n",
      "0.15852470695972443\n",
      "0.16841255128383636\n",
      "0.12325353175401688\n",
      "0.17164871096611023\n",
      "0.11731407791376114\n",
      "0.13896362483501434\n",
      "0.15644870698451996\n",
      "0.19016943871974945\n",
      "0.1252375841140747\n",
      "0.12235753238201141\n",
      "0.19060474634170532\n",
      "0.1451263427734375\n",
      "0.14474700391292572\n",
      "0.10673528164625168\n",
      "0.14978279173374176\n",
      "0.14176325500011444\n",
      "0.3671860992908478\n",
      "0.09850433468818665\n",
      "0.10364632308483124\n",
      "0.1017536073923111\n",
      "0.17692404985427856\n",
      "0.1473563015460968\n",
      "0.1603533923625946\n",
      "0.16521289944648743\n",
      "0.41557779908180237\n",
      "0.15970104932785034\n",
      "0.13468188047409058\n",
      "0.16487225890159607\n",
      "0.1093960851430893\n",
      "0.1298566460609436\n",
      "0.16987726092338562\n",
      "0.20244121551513672\n",
      "0.15297730267047882\n",
      "0.14196555316448212\n",
      "0.16539902985095978\n",
      "0.0974259153008461\n",
      "0.17817097902297974\n",
      "0.09636978060007095\n",
      "0.13273225724697113\n",
      "0.12239508330821991\n",
      "0.11154618859291077\n",
      "0.19544224441051483\n",
      "0.10964011400938034\n",
      "0.4607158601284027\n",
      "0.19505125284194946\n",
      "0.185074120759964\n",
      "0.1130450963973999\n",
      "0.1948476880788803\n",
      "0.1904052346944809\n",
      "0.20557737350463867\n",
      "0.1380159705877304\n",
      "0.11816252022981644\n",
      "0.09369923919439316\n",
      "0.2325737178325653\n",
      "0.41986367106437683\n",
      "0.16194401681423187\n",
      "0.17005419731140137\n",
      "0.12876693904399872\n",
      "0.1498441845178604\n",
      "0.13699448108673096\n",
      "0.3377898335456848\n",
      "0.14318908751010895\n",
      "0.13483263552188873\n",
      "0.15376032888889313\n",
      "0.13304226100444794\n",
      "0.18340212106704712\n",
      "0.12024442106485367\n",
      "0.10009663552045822\n",
      "0.4142402708530426\n",
      "0.15635091066360474\n",
      "0.12001074105501175\n",
      "0.1589057296514511\n",
      "0.12874814867973328\n",
      "0.2023923099040985\n",
      "0.15265367925167084\n",
      "0.148814857006073\n",
      "0.3115648925304413\n",
      "0.12549854815006256\n",
      "0.1369064450263977\n",
      "0.13847997784614563\n",
      "0.1771954745054245\n",
      "0.11609011143445969\n",
      "0.22873084247112274\n",
      "0.15744079649448395\n",
      "0.20704789459705353\n",
      "0.16458125412464142\n",
      "0.11352662742137909\n",
      "0.14818567037582397\n",
      "0.17435522377490997\n",
      "0.17048093676567078\n",
      "0.4464026093482971\n",
      "0.16854381561279297\n",
      "0.13866902887821198\n",
      "0.1633412092924118\n",
      "0.24572919309139252\n",
      "0.12242656946182251\n",
      "0.38579320907592773\n",
      "0.1650415062904358\n",
      "0.16650845110416412\n",
      "0.12036619335412979\n",
      "0.09130573272705078\n",
      "0.18576107919216156\n",
      "0.16738571226596832\n",
      "0.10557082295417786\n",
      "0.14789824187755585\n",
      "0.12956750392913818\n",
      "0.14894001185894012\n",
      "0.23845605552196503\n",
      "0.18086974322795868\n",
      "0.22079716622829437\n",
      "0.17550186812877655\n",
      "0.188791885972023\n",
      "0.14236104488372803\n",
      "0.2690029740333557\n",
      "0.13657861948013306\n",
      "0.43631651997566223\n",
      "0.1828000545501709\n",
      "0.13585324585437775\n",
      "0.17087416350841522\n",
      "0.1497846245765686\n",
      "0.11088666319847107\n",
      "0.15246598422527313\n",
      "0.22192171216011047\n",
      "0.13789710402488708\n",
      "0.24299344420433044\n",
      "0.12328668683767319\n",
      "0.140810027718544\n",
      "0.12511757016181946\n",
      "0.18548816442489624\n",
      "0.1376459151506424\n",
      "0.18323291838169098\n",
      "0.13739927113056183\n",
      "0.16130082309246063\n",
      "0.1344081163406372\n",
      "0.13733656704425812\n",
      "0.16597557067871094\n",
      "0.1938745379447937\n",
      "0.11468788236379623\n",
      "0.10756596177816391\n",
      "0.14241886138916016\n",
      "0.11685553938150406\n",
      "0.13478167355060577\n",
      "0.17417050898075104\n",
      "0.15267428755760193\n",
      "0.137849360704422\n",
      "0.12430784106254578\n",
      "0.36606088280677795\n",
      "0.10989344120025635\n",
      "0.12560616433620453\n",
      "0.10055121034383774\n",
      "0.19799228012561798\n",
      "0.3544274568557739\n",
      "0.12613992393016815\n",
      "0.36206087470054626\n",
      "0.14645645022392273\n",
      "0.1825687289237976\n",
      "0.4198804497718811\n",
      "0.15035445988178253\n",
      "0.15311367809772491\n",
      "0.2199215590953827\n",
      "0.14164909720420837\n",
      "0.3958719074726105\n",
      "0.11642127484083176\n",
      "0.15050344169139862\n",
      "0.13630801439285278\n",
      "0.15011733770370483\n",
      "0.13454177975654602\n",
      "0.1185017004609108\n",
      "0.1136021763086319\n",
      "0.11967449635267258\n",
      "0.06243516877293587\n",
      "Epoch 5/10, Loss: 0.17207398028671742\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "train_loss_history, val_loss_history =train_module(modelf)\n",
    "# predictions: [num_patients, num_diseases] tensor\n",
    "# targets: multi-hot [num_patients, num_diseases] tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, 5 + 1) # Epoch numbers for x-axis\n",
    "\n",
    "plt.figure(figsize=(10, 6)) # Optional: Adjust figure size\n",
    "\n",
    "plt.plot(epochs, train_loss_history, label='Training Loss', marker='o') # Plot training loss\n",
    "plt.plot(epochs, val_loss_history, label='Validation Loss', marker='x')   # Plot validation loss\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curves: Training and Validation Loss')\n",
    "plt.legend() # Show legend to distinguish lines\n",
    "plt.grid(True) # Optional: Add grid lines\n",
    "plt.xticks(epochs) # Ensure x-axis ticks are at each epoch\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "plt.show() # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Recall: 0.285, Cumulative Precision: 0.285\n",
      "Recall@4: True Positives: 16247.0, False Positives: 17293.0, False Negatives: 328635.0\n",
      "Cumulative Evaluation Metrics @4:\n",
      "recall@4: 0.0520\n",
      "precision@4: 0.4844\n",
      "ndcg@4: 0.5116\n",
      "map@4: 0.0392\n"
     ]
    }
   ],
   "source": [
    "#eval_module(patient_inputs_val,patient_targets_val, modelf,global_graph)# After training:\n",
    "val_metrics = enhanced_eval_module(\n",
    "    model=modelf,\n",
    "    patient_inputs_val=val_inputs,\n",
    "    targets_multi_hot=val_targets_multi_hot,\n",
    "    global_graph=global_graph,\n",
    "    k=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
